---
title: "CFB"
output: html_document
date: "2022-09-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(Hmisc)
library(cfbfastR)
```

```{r}
install.packages("cfbfastR")
```

```{r}
install.packages("tictoc")
```

```{r}
tictoc::tic()
pbp <- data.frame()
seasons <- 2014:cfbfastR:::most_recent_cfb_season()
progressr::with_progress({

  pbp <- cfbfastR::load_cfb_pbp(seasons)
})
tictoc::toc()
```

```{r}
glimpse(pbp)
```

```{r}
#which team has had the most penalties on the first play of a series?
pbp %>%
  filter(new_series == 1, drive_play_number == 1, play_type == 'Penalty', down == 1) %>%
  group_by(year) %>%
  summarize(plays = n(), games = n_distinct(game_id)) %>%
  arrange(desc(plays))
```

#College Football Regression Analysis Assignment

```{r}
logs <- read_csv("http://mattwaite.github.io/sportsdatafiles/footballlogs1121.csv")
  
head(logs)

logs <- logs %>%
  mutate(PointDifferential = TeamScore - OpponentScore)

#Create a regression investigating whether the number of penalties can predict the score differential.
fit <- lm(PointDifferential ~ Penalties, data = logs)
summary(fit)
```
  
#In a paragraph below, describe the results: what is the p-value, and does it mean the results are random? Using the r-squared value, how much of the differential can be explained by penalty yards? How useful is this regression?

p-value: 0.01856
R-squared: 0.0002627 
Because our p-value is less than 0.05, we know that the relationship between point differential and penalties is statistically significant, or not the relationship is not random. But when we turn to the r-squared value, which measures the percent of how much the two are related, we see that it is less than 1%, so point differential and penalties aren't related at all. In other words, there is less than 1% of the difference in differential with the penalties. So, to answer the question of how much of the differential can be explained by penalty yards? Not much, really. This regression is not that helpful because it turns out that our two variables in this case are not that related to each other. Also of note, the residual standard error is 22.76 (meaning that our predictive range is pretty large), which tells us that there is a decent amount of error in this model. And one more thing: the min/max residual values are -80 to 81, meaning that a team underperformed the model's prediction by 80 and one team overperformed by 81, so it missed games by those values when trying to predict certain outcomes and scores. The farther apart our residual values are, the worse the model is.

#Next, create a multiple regression model. Instead of using the number of penalties alone, combine other numeric columns that describe mistakes/bad outcomes to try and predict the score differential with a higher degree of confidence. Look at the same values in the results you did before, but also take into account the residual standard error and the risk of multicollinearity - are you adding columns that explain each other instead of the differential? Below this code block, explain your choices and what you think the results say.

```{r}
#I was curious about the differentials for all the "bad outcome" columns.
logs <- logs %>%
  mutate(
    FirstDownPenDifferential = FirstDownPen - DefFirstDownPen,
    PenYdsDifferential = PenaltyYds - DefPenaltyYds,
    PenDifferential = Penalties - DefPenalties,
    FumbleDifferential = Fumbles - DefFumbles,
    InterceptionsDifferential = Interceptions - DefInterceptions,
    TurnoverDifferential = TotalTurnovers - DefTotalTurnovers,
    YardsDifferential = OffensiveYards - DefYards)

#Turnovers are bad
model1 <- lm(PointDifferential ~ InterceptionsDifferential + FumbleDifferential, data=logs)
summary(model1)
#p-value: 2.2e-16
#R-squared: 0.2499 
#Min residual: -79.612
#Max residual: 78.996
#RSE: 19.72

#My thought process here is that int and fum are two of the worst outcomes you can have, so they must be related to point differential. I didn't throw in turnover differential because it stands to reason that TO are made up of int and fum. In other words, having a model with all three would just be adding columns that explain each other instead of the differential. What I learned: Our p-value shows that this is not random, and our r-squared predicts about a quarter of the the factors that impact point differential can be attributed to int and fum. Where we get into trouble is with the min/max residuals and RSE. the m/m values are really far apart, and we have a large range of residual standard error. So while the thought behind this model was sound, it isn't the strongest.

#Turnovers and penalty yards are worse...?
model2 <- lm(PointDifferential ~ TurnoverDifferential + PenYdsDifferential, data=logs)
summary(model2)
#p-value: 2.2e-16
#R-squared: 0.2321 
#Min residual: -79.721
#Max residual: 82.474
#RSE: 19.95

#My thinking here was that turnovers (int/fum) are all-encompassing, so pairing that with penalty yards (another total) would cover all the major parts of "bad outcomes." Similarly to model1, model2 tells us that the relationship btwn the two is not random and that there is about a 25% predicted outcome of TO and penalty yards when looking at point differential. Again, though, our m/m residual values are super far apart and the RSE is also almost 20. So, yet again, solid thinking doesn't quite pan out here.

#INT, FUM, PENYDS must be the worst, right?
model3 <- lm(PointDifferential ~  InterceptionsDifferential + FumbleDifferential + PenYdsDifferential, data=logs)
summary(model3)
#p-value: 2.2e-16
#R-squared: 0.2499 
#Min residual: -79.670
#Max residual: 79.104
#RSE: 19.72

#I was really excited about this one. This is the Bermuda Triangle of "bad outcomes." I mean, c'mon! Certainly int + fum + penyards is a deadly outcome, so this must be predictive of point differential. Sadly, though, model3 looks a lot like models one and two. It's not random, has a 25% r-squared value, large residuals and a pretty crummy RSE. One thing to point out here, and although I joked about it up top, I was really surprised this model wasn't more revealing. Reason says that this combination should have an impact on point differential, so it's weird that it looks a lot like our previous models. Now, it is similar in code to model1, but I thought adding in penalty yards would have a greater effect than it did. Almost all the numbers are identical. Looking at that, it looks like penalty yards don't add that much more to our model. And while I don't think this means penalty yards are inconsequential when looking at point differential, it is curious that its impact was negligible when paired with int/fum. 
```

#Use filter to narrow the game data so that you're only working with games that are close (you'll need to define what "close" means). Are your simple or multiple regression models better? Worse? Below this code block, explain your choices and what you think the results say.

```{r}
#Here, I defined "close games" as one-score matches, so eight points, assuming a team scored a touchdown and converted the two-point try.
close_games <- logs %>%
  filter(PointDifferential < 9, PointDifferential >-9)

model_close_games <- lm(PointDifferential ~ InterceptionsDifferential + FumbleDifferential, data=close_games)
summary(model_close_games)
#p-value: 2.2e-16
#R-squared: 0.09807 
#Min residual: -11.2655
#Max residual: 11.1263
#RSE: 4.789

#Wishing for these models to be better is like wishing you could go trick-or-treating as a 25-year-old. When we filtered for "close games" — defined above — using int/fum to see the explain point differential, we saw that the outcome wasn't random, our RSE dropped from 20 to four, so that was super encouraging, and we saw that the min/max residual values had gone from a 150-point gap to a 22-point gap. This is looking pretty good (giddy excitement builds). But then we go and look at our r-squared value and see that it comes in at only 10% :-(. Darn it! 

#With everything else looking so good, it's frustrating to see that our r-value is lower than our other models. Reason says that int/fum should be super important in close games. In some cases, a turnover might be the difference in a "close game," so why is it that this model only explains 10% of point differential? 

#While I don't have a good answer, I suppose that these models are using all the "bad outcomes" to try and predict point differential, and it's entirely possible that we need to include offensive metrics as well. Maybe a combination of boneheaded plays on defense and languid offense negatively impacts point differential, or perhaps if you have Carson Wentz hurling the ball to Terry McLaurin (on a good day) combined with what Washington's defense is supposed to be, point differential will be positively impacted. 

#Maybe we should be looking at some sort of combination of offensive and defensive metrics. Football is won on both sides of the ball, after all. 

model_close_games_with_offense <- lm(PointDifferential ~ InterceptionsDifferential + FumbleDifferential + YardsDifferential, data=close_games)
summary(model_close_games_with_offense)
#p-value: 2.2e-16
#R-squared: 0.1841 
#Min residual: -12.3991
#Max residual: 12.2673
#RSE: 4.555

#Just cause I was curious, I wanted to try one offensive metric, and the early return seem to indicate that adding offensive stats might help our model. This one looks similar to our other model_close_game models, but the r-value doubled, almosst ten points, so maybe there is something there.

#Based on the results we got from the simple and multiple regression models, the multiple regression models are stronger. At least in the multiple regression models our r-value was a somewhat significant number. In the single regression model, our r-squared was below 1%...yikes. In terms of which of the multiple regression models was best, the model_close_game models seemed to be a bit better than models one, two and three. I say this because while the r-value wasn't as good, our residual values and RSE were drastically improved, meaning there was less error in our model, even though it explained less of the total of point differential.

model_close_games2 <- lm(PointDifferential ~ InterceptionsDifferential + FumbleDifferential + PenYdsDifferential, data=close_games)
summary(model_close_games2)
#p-value: 2.2e-16
#R-squared: 0.09827 
#Min residual: -11.3417
#Max residual: 11.2030
#RSE: 4.788

#Even though we learned above that adding penalty yards to a model with int/fum doesn't really do much, like Mr. Run The Ball guy, I wanted to test it out jsut to be sure. And sure enough, we get almost identical numbers to model_close_games. Nothing more to add here, folks.
```

#At the end of all that code, summarize what you've learned about the relationship between penalties and point differential and whether you think there's a story there or whether it's useful in adding context within a larger story. Would you use this in journalism and, if so, how?

After all this code, I don't believe there is a strong relationship between penalties and point differential. I say this because all our models are weak; they did not do a good job of giving us results that explain the relationship between point differential and penalties. Based on our results, there just isn't much there. And while there might not be a strong story here, it could be useful in adding context within a larger story if we were able to do some more analysis (adding in offensive numbers, which seems to be worth exploring). 

I wouldn't feel confident using these results in journalism or pitching this as a potential story to an editor, but I think there are some interesting concepts — are penalties not that important? — to keep in mind for further investigations. 

And if we do go a step further and find out that penalties really aren't that important in explaining point differential (and I happened to be on the Maryland football team), I would go back to the first practice after the SMU game and present my findings to coach Locksley!